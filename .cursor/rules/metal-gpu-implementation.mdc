# Metal GPU Implementation Guidelines for NumSwift

## Overview

This document provides comprehensive guidelines for working with NumSwift's Metal GPU backend, including compute shader development, performance optimization, and integration patterns.

## Metal Architecture

### Three-Layer Design
1. **Metal Compute Shaders** (`NumSwiftMetal.metal`): GPU kernels for all operations
2. **Swift Metal Wrapper** (`NumSwiftMetal.swift`): Backend management and automatic selection
3. **Unified API**: Seamless interface that abstracts CPU/GPU differences

### Core Components
- **MetalConfiguration**: Device and resource management
- **NumSwiftMetal**: Main backend class with automatic switching
- **Compute Pipeline Cache**: Optimized shader compilation and reuse
- **Buffer Pool**: Memory management for frequent operations

## Compute Shader Development

### Shader Categories

#### Basic Array Operations
- **Reduction Operations**: sum, max, min with parallel reduction algorithms
- **Element-wise Operations**: add, multiply, divide with full parallelization
- **Utility Operations**: copy, zero, type conversion

#### Matrix Operations
- **Matrix Multiplication**: Simple and tiled implementations for different sizes
- **Transpose**: Efficient memory access patterns
- **Batched Operations**: Neural network optimizations

#### Convolution Operations
- **2D Convolution**: Standard and im2col implementations
- **1D Convolution**: Optimized for signal processing
- **Transpose Convolution**: Upsampling operations
- **Padding Operations**: Zero padding and stride padding

### Shader Writing Guidelines

#### Thread Dispatch Patterns
```metal
// 1D operations - linear dispatch
kernel void operation_1d_kernel(device const float* input [[ buffer(0) ]],
                                device float* result [[ buffer(1) ]],
                                uint id [[ thread_position_in_grid ]]) {
    result[id] = process(input[id]);
}

// 2D operations - 2D dispatch
kernel void operation_2d_kernel(device const float* input [[ buffer(0) ]],
                                device float* result [[ buffer(1) ]],
                                device const NSC_Size* size [[ buffer(2) ]],
                                uint2 id [[ thread_position_in_grid ]]) {
    uint row = id.y, col = id.x;
    if (row >= size->rows || col >= size->columns) return;
    // Process element at [row][col]
}
```

#### Memory Access Optimization
- **Coalesced Access**: Ensure adjacent threads access adjacent memory
- **Shared Memory**: Use threadgroup memory for data reuse
- **Buffer Alignment**: Align data structures for optimal GPU access

#### Data Type Support
```metal
// Support both float and half precision
kernel void operation_float_kernel(device const float* input [[ buffer(0) ]],
                                   device float* result [[ buffer(1) ]],
                                   uint id [[ thread_position_in_grid ]]);

kernel void operation_half_kernel(device const half* input [[ buffer(0) ]],
                                  device half* result [[ buffer(1) ]],
                                  uint id [[ thread_position_in_grid ]]);
```

## Swift Integration Patterns

### Backend Selection Logic
```swift
private func shouldUseMetal(for elementCount: Int) -> Bool {
    switch backend {
    case .cpu:
        return false
    case .metal:
        return true
    case .auto:
        // Operation-specific thresholds
        return elementCount > getThreshold(for: operationType)
    }
}
```

### Operation Implementation Pattern
```swift
public func operation<T>(_ input: [T]) -> [T] {
    // Validate inputs
    guard !input.isEmpty else { return [] }
    
    // Choose backend
    if shouldUseMetal(for: input.count) {
        if let result = metalOperation(input) {
            return result
        }
    }
    
    // Fallback to CPU
    return cpuOperation(input)
}
```

### Buffer Management
```swift
private func createBuffer<T>(from data: [T], type: T.Type) -> MTLBuffer? {
    let size = data.count * MemoryLayout<T>.stride
    
    // Try buffer pool first
    if let buffer = bufferPool.getBuffer(size: size) {
        copyData(data, to: buffer)
        return buffer
    }
    
    // Create new buffer with shared storage
    return device.makeBuffer(bytes: data, length: size, options: .storageModeShared)
}
```

## Performance Optimization

### Thread Group Sizing
```swift
private func getOptimalThreadgroupSize(width: Int, height: Int) -> MTLSize {
    // Apple Silicon optimizations
    if device.name.contains("Apple") {
        if width * height > 100000 {
            return MTLSize(width: 32, height: 32, depth: 1)
        } else {
            return MTLSize(width: 16, height: 16, depth: 1)
        }
    }
    
    // Conservative sizing for other GPUs
    return MTLSize(width: 8, height: 8, depth: 1)
}
```

### Memory Optimization
- **Buffer Pooling**: Reuse buffers to reduce allocation overhead
- **Shared Storage**: Use `.storageModeShared` for CPU-GPU data sharing
- **Async Operations**: Overlap CPU and GPU work when possible

### Hardware-Specific Optimizations
- **Apple Silicon**: Leverage unified memory architecture
- **Discrete GPUs**: Minimize data transfer between CPU and GPU
- **Thread Limits**: Respect hardware-specific thread group limits

## Testing and Validation

### Correctness Testing
```swift
func testOperationCorrectness() {
    let testData = generateTestData()
    
    // Test CPU implementation
    backend.setBackend(.cpu)
    let cpuResult = backend.operation(testData)
    
    // Test GPU implementation
    backend.setBackend(.metal)
    let gpuResult = backend.operation(testData)
    
    // Validate results match within tolerance
    XCTAssertEqual(cpuResult, gpuResult, accuracy: 1e-5)
}
```

### Performance Benchmarking
```swift
func benchmarkOperation() {
    let sizes = [100, 1000, 10000, 100000]
    
    for size in sizes {
        let data = Array(1...size).map { Float($0) }
        
        // Benchmark CPU
        let cpuTime = measureTime {
            backend.setBackend(.cpu)
            _ = backend.operation(data)
        }
        
        // Benchmark GPU
        let gpuTime = measureTime {
            backend.setBackend(.metal)
            _ = backend.operation(data)
        }
        
        print("Size: \(size), CPU: \(cpuTime)ms, GPU: \(gpuTime)ms, Speedup: \(cpuTime/gpuTime)x")
    }
}
```

## Error Handling and Debugging

### Metal Error Handling
```swift
private func executeKernel(pipeline: MTLComputePipelineState, 
                          buffers: [MTLBuffer],
                          threadgroupSize: MTLSize,
                          threadgroupCount: MTLSize) -> Bool {
    guard let commandBuffer = commandQueue.makeCommandBuffer(),
          let encoder = commandBuffer.makeComputeCommandEncoder() else {
        print("Failed to create command buffer or encoder")
        return false
    }
    
    encoder.setComputePipelineState(pipeline)
    
    for (index, buffer) in buffers.enumerated() {
        encoder.setBuffer(buffer, offset: 0, index: index)
    }
    
    encoder.dispatchThreadgroups(threadgroupCount, threadsPerThreadgroup: threadgroupSize)
    encoder.endEncoding()
    
    commandBuffer.commit()
    commandBuffer.waitUntilCompleted()
    
    if let error = commandBuffer.error {
        print("Metal execution error: \(error)")
        return false
    }
    
    return true
}
```

### Debugging Tips
- **Metal Debugger**: Use Xcode's Metal debugger for kernel inspection
- **GPU Timeline**: Profile kernel execution times
- **Memory Validation**: Check for buffer overruns and memory leaks
- **Numerical Accuracy**: Validate GPU results against CPU reference

## Best Practices

### Development Workflow
1. **Implement CPU version first** for correctness reference
2. **Write Metal kernel** with proper bounds checking
3. **Create Swift wrapper** with automatic backend selection
4. **Add comprehensive tests** for correctness and performance
5. **Profile and optimize** based on real-world usage

### Code Organization
- Keep Metal shaders in `NumSwiftMetal.metal`
- Implement Swift wrappers in `NumSwiftMetal.swift`
- Use consistent naming: `nsc_operation_kernel` for Metal, `metalOperation` for Swift
- Document performance characteristics and optimal usage patterns

### Performance Guidelines
- **Profile before optimizing**: Measure actual performance gains
- **Consider overhead**: GPU setup cost vs computation benefit
- **Test on target hardware**: Performance varies significantly between devices
- **Validate accuracy**: Ensure numerical results match CPU implementation

## Future Considerations

### Scalability
- **Multi-GPU Support**: Prepare for systems with multiple Metal devices
- **Async Operations**: Implement truly asynchronous GPU operations
- **Streaming**: Handle datasets larger than GPU memory

### Advanced Features
- **Custom Operators**: Framework for user-defined Metal operations
- **Neural Network Layers**: Optimized implementations for common ML operations
- **Tensor Operations**: Higher-dimensional array support

This document should be updated as the Metal implementation evolves and new optimization opportunities are discovered.